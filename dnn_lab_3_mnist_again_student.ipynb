{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalSlowakiewicz/Deep-Neural-Network/blob/master/dnn_lab_3_mnist_again_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84VetyCaGLyR"
      },
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziZ9i7tXbO1T"
      },
      "source": [
        "In this lab, you will implement some of the techniques discussed in the lecture.\n",
        "\n",
        "Below you are given a solution to the previous scenario. It has two serious drawbacks:\n",
        " * The output predictions do not sum up to one (i.e. the output is not a probability distribution), even though the images always contain exactly one digit.\n",
        " * It uses MSE coupled with output sigmoid, which can lead to saturation and slow convergence.\n",
        "\n",
        "**Task 0.** Implement a numerically stable version of softmax.\n",
        "\n",
        "**Task 1.** Use softmax instead of coordinate-wise sigmoid and use log-loss instead of MSE. Test to see if this improves convergence. Hint: When implementing backprop it might be easier to consider these two functions as a single block, rather than compute the gradient over the softmax values.\n",
        "\n",
        "**Task 2.** Implement L2 regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.\n",
        "\n",
        "**Task 3 (optional).** Implement Adagrad or AdamW (currently popular in LLM training), dropout, and some simple data augmentations (e.g. tiny rotations/shifts, etc.). Again, test to see how these changes improve accuracy/convergence.\n",
        "\n",
        "**Task 4.** Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence. As a start, you can try this architecture: [784,100,30,10]\n",
        "\n",
        "The provided model evaluation code (`evaluate_model`) may take some time to complete. During implementation, you can change the number of evaluated models to 1 and reduce the number of tested learning rates, and epochs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iZypYewcXywA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe7434b-4add-4719-e1f8-bf3ce7b1c7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "2025-10-17 08:27:52 URL:https://s3.amazonaws.com/img-datasets/mnist.npz [11490434/11490434] -> \"mnist.npz\" [1]\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm pandas\n",
        "!wget --no-verbose -O mnist.npz https://s3.amazonaws.com/img-datasets/mnist.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "P22HqX9AbO1a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Callable, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from numpy.typing import NDArray\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "FloatNDArray = NDArray[np.float64]\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N9jGPaZhbO2B"
      },
      "outputs": [],
      "source": [
        "def load_mnist(\n",
        "    path: Path = Path(\"mnist.npz\")\n",
        ") -> tuple[FloatNDArray, FloatNDArray, FloatNDArray, FloatNDArray]:\n",
        "    \"\"\"\n",
        "    Load the MNIST dataset (grayscale 28 x 28 images of hand-written digits).\n",
        "\n",
        "    Returns tuple of:\n",
        "    - x_train: shape (N_train, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_train, 10), one-hot-encoded label, dtype float64.\n",
        "    - x_test: shape (N_test, H * W), grayscale values 0..1.\n",
        "    - y_train: shape (N_test, 10), one-hot-encoded label, dtype float64.\n",
        "\n",
        "    More: https://en.wikipedia.org/wiki/MNIST_database\n",
        "    \"\"\"\n",
        "    with np.load(path) as f:\n",
        "        x_train, _y_train = f[\"x_train\"], f[\"y_train\"]\n",
        "        x_test, _y_test = f[\"x_test\"], f[\"y_test\"]\n",
        "\n",
        "    H = W = 28\n",
        "    N_train = len(x_train)\n",
        "    N_test = len(x_test)\n",
        "    assert x_train.shape == (N_train, H, W) and _y_train.shape == (N_train,)\n",
        "    assert x_test.shape == (N_test, H, W) and _y_test.shape == (N_test,)\n",
        "\n",
        "    x_train = x_train.reshape(N_train, H * W) / 255.0\n",
        "    x_test = x_test.reshape(N_test, H * W) / 255.0\n",
        "\n",
        "    y_train = np.zeros((N_train, 10), dtype=np.float64)\n",
        "    y_train[np.arange(N_train), _y_train] = 1\n",
        "\n",
        "    y_test = np.zeros((N_test, 10))\n",
        "    y_test[np.arange(N_test), _y_test] = 1\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "x_train, y_train, x_test, y_test = load_mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w3gAyqw4bO1p"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: FloatNDArray) -> FloatNDArray:\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def sigmoid_prime(z: FloatNDArray) -> FloatNDArray:\n",
        "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "    return sigmoid(z) * (1 - sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvVG90fCXywB"
      },
      "source": [
        "## Warm-Up\n",
        "Implement a numerically stable version of softmax.  \n",
        "\n",
        "In general, softmax is defined as  \n",
        "$$\\text{softmax}(x_1, x_2, \\ldots, x_n) = (\\frac{e^{x_1}}{\\sum_i{e^{x_i}}}, \\frac{e^{x_2}}{\\sum_i{e^{x_i}}}, \\ldots, \\frac{e^{x_n}}{\\sum_i{e^{x_i}}})$$  \n",
        "However, taking $e^{1000000}$ can result in NaN.  \n",
        "Can you implement softmax so that the highest power to which e will be risen will be at most $0$ and the predictions will be mathematically equivalent?  \n",
        "\n",
        "Hint: <sub><sub><sub>sǝnlɐʌ llɐ ɯoɹɟ ʇᴉ ʇɔɐɹʇqns  puɐ ᴉ‾x ʇsǝƃɹɐl ǝɥʇ ǝʞɐʇ</sub></sub></sub>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_rutQhoaXywC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "226a5a22-c029-4ef3-baee-fd22fe5902bc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3284601720.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtest_stable_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m### TESTS END ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3284601720.py\u001b[0m in \u001b[0;36mtest_stable_softmax\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_stable_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0m_test_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munstable_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3284601720.py\u001b[0m in \u001b[0;36m_test_one\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstable_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Expected shape {y.shape}, got {r.shape=}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def unstable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
        "    e = np.exp(x)\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "def stable_softmax(x: FloatNDArray, axis: int = -1) -> FloatNDArray:\n",
        "    ## TODO\n",
        "    ###{\n",
        "    max_x = np.max(x)\n",
        "    e = (x - max_x)\n",
        "    e = np.exp(e)\n",
        "    return e / np.sum(e, axis=axis, keepdims=True)\n",
        "    ###}\n",
        "\n",
        "\n",
        "### TESTS ###\n",
        "def _test_one(x: FloatNDArray, y: FloatNDArray) -> None:\n",
        "    r = stable_softmax(x)\n",
        "    assert r.shape == y.shape, f\"Expected shape {y.shape}, got {r.shape=}\"\n",
        "    assert np.isclose(np.ones(x.shape[0]), r.sum(axis=-1), atol=1e-5, rtol=0).all()\n",
        "    assert np.isclose(y, r, atol=1e-5, rtol=0).all()\n",
        "\n",
        "def test_stable_softmax() -> None:\n",
        "    x1 = np.random.rand(100, 32).astype(np.float64)\n",
        "    _test_one(x1, unstable_softmax(x1))\n",
        "\n",
        "    x2 = np.ones((10, 10, 32), dtype=np.float64) * 1e6\n",
        "    _test_one(x2, np.ones_like(x2) / x2.shape[-1])\n",
        "\n",
        "    print(\"OK\")\n",
        "\n",
        "test_stable_softmax()\n",
        "### TESTS END ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ModelResults\n"
      ],
      "metadata": {
        "id": "WWFnNWF8fWJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FgEA2XRRbO2X"
      },
      "outputs": [],
      "source": [
        "class ModelResults:\n",
        "    \"\"\"Just a helper class for gathering results in a nice table. Feel free to ignore.\"\"\"\n",
        "    def __init__(self):\n",
        "        # Map from model name to map from lr to list of test accuracies.\n",
        "        self.results = dict[str, dict[float, list[float]]]()\n",
        "\n",
        "    def clear(self, model_name: str | None = None) -> None:\n",
        "        \"\"\"Forget results for a given model (defaults to all models).\"\"\"\n",
        "        if model_name:\n",
        "            if model_name in self.results:\n",
        "                del self.results[model_name]\n",
        "        else:\n",
        "            self.results = {}\n",
        "\n",
        "    def add_result(self, model_name: str, learning_rate: float, accuracy: float) -> None:\n",
        "        if model_name not in self.results:\n",
        "            self.results[model_name] = {}\n",
        "        if learning_rate not in self.results[model_name]:\n",
        "            self.results[model_name][learning_rate] = []\n",
        "        self.results[model_name][learning_rate].append(accuracy)\n",
        "\n",
        "    def display_results(self) -> None:\n",
        "        data = list[dict[str, Any]]()\n",
        "        for model_name, model_results in self.results.items():\n",
        "            for lr, accuracies in model_results.items():\n",
        "                mean_accuracy = np.mean(accuracies)\n",
        "                accuracy_summary = f\"{mean_accuracy:2.1%} ± {np.std(accuracies) * 100:.1f} p.p.\"\n",
        "                data.append({\n",
        "                    \"model\": model_name,\n",
        "                    \"lr\": lr,\n",
        "                    \"mean_accuracy\": mean_accuracy,\n",
        "                    \"accuracy\": accuracy_summary\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(data).sort_values(\"mean_accuracy\", ascending=False)\n",
        "        del df[\"mean_accuracy\"]\n",
        "        display(df.style.format({\"lr\": \"{:.1g}\"}).hide())\n",
        "\n",
        "    def evaluate_model(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        model_constructor: Callable[[Sequence[int]], Any],\n",
        "        layers: Sequence[int] = (784, 30, 10),\n",
        "        learning_rates: Sequence[float] = (1.0, 10.0, 100.0),\n",
        "        n_trainings: int = 3,\n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        # Automatic model name with parameters.\n",
        "        if kwargs:\n",
        "            if tuple(layers) != (784, 30, 10):\n",
        "                model_name += \"[\" + \",\".join(str(n) for n in layers) + \"]\"\n",
        "\n",
        "            model_name += \"(\"\n",
        "            for k, v in kwargs.items():\n",
        "                if isinstance(v, (float,  np.floating)):\n",
        "                    model_name += f\"{k}={v:.1g},\"\n",
        "                else:\n",
        "                    model_name += f\"{k}={v},\"\n",
        "            model_name = model_name[:-1]\n",
        "            model_name += \")\"\n",
        "\n",
        "        # Train for each learning rate, n_trainings times.\n",
        "        for lr in learning_rates:\n",
        "            print(f\"Checking {n_trainings} random trainings with with lr = {lr}\")\n",
        "            for i in range(n_trainings):\n",
        "                network = model_constructor(layers, **kwargs)\n",
        "                accuracy = network.train(\n",
        "                    (x_train, y_train),\n",
        "                    epochs=10,\n",
        "                    mini_batch_size=100,\n",
        "                    learning_rate=lr,\n",
        "                    test_data=(x_test, y_test),\n",
        "                )\n",
        "                self.add_result(model_name, lr, float(accuracy))\n",
        "\n",
        "\n",
        "model_results = ModelResults()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "The solution to the previous lab: an MLP network with MSE loss on sigmoid outputs, trained with plain SGD (batched)."
      ],
      "metadata": {
        "id": "kJD-EMvq6H2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self, sizes: Sequence[int] = (784, 30, 10)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "        - sizes: sequence of layer widths [N^0, ... , N^last]\n",
        "          These are lengths of activation vectors, where:\n",
        "          - N^0 is input size: H * W = 28 * 28 = 784.\n",
        "          - N^last is the number of classes into which we can classify each input: 10.\n",
        "        \"\"\"\n",
        "        self.sizes = list(sizes)\n",
        "\n",
        "        # List of len(sizes) - 1 vectors of shape (N^1), (N^2), ..., (N^last).\n",
        "        self.biases = [np.random.randn(n) for n in sizes[1:]]\n",
        "\n",
        "        # List of len(sizes) - 1 matrices of shape (N^i, N^{i-1}).\n",
        "        # Weights are indexed by target node first.\n",
        "        self.weights = [\n",
        "            np.random.randn(n_out, n_in) / np.sqrt(n_in)\n",
        "            for n_in, n_out in zip(sizes[:-1], sizes[1:], strict=True)\n",
        "        ]\n",
        "\n",
        "        self.num_layers = len(self.weights)   # = len(sizes) - 1\n",
        "\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"\n",
        "        Run the network on a batch of cases of shape (B, N^0), values 0..1.\n",
        "\n",
        "        Returns last layer activations, shape (B, N^last), values 0..1.\n",
        "        \"\"\"\n",
        "        g = x\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            # Shapes (B, N^{i-1}) @ (N^{i-1}, N^i).T + (N^i,)  ==  (B, N^i)\n",
        "            f = g @ w.T + b\n",
        "            g = sigmoid(f)\n",
        "        return g\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update network parameters with a single mini-batch step of backpropagation and gradient descent.\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        # Gradient descent step.\n",
        "        self.weights = [\n",
        "            w - learning_rate * grad_w\n",
        "            for w, grad_w in zip(self.weights, grads_w, strict=True)\n",
        "        ]\n",
        "        self.biases = [\n",
        "            b - learning_rate * grad_b\n",
        "            for b, grad_b in zip(self.biases, grads_b, strict=True)\n",
        "        ]\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        \"\"\"\n",
        "        Backpropagation for a mini-batch (vectorized).\n",
        "\n",
        "        Args:\n",
        "        - x: input, shape (B, N^0)\n",
        "        - y: target label (one-hot encoded), shape (B, N^last)\n",
        "\n",
        "        Returns (grads_w, grads_b), where:\n",
        "        - grads_w: list of gradients over weights (shape (N^i, N^{i-1})), for each layer.\n",
        "        - grads_b: list of gradients over biases (shape (N^i)), for each layer.\n",
        "        \"\"\"\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "\n",
        "\n",
        "        # Forward pass.\n",
        "        fs, gs = [], [x]\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            f = gs[-1] @ w.T + b\n",
        "            fs.append(f)\n",
        "            g = sigmoid(f)\n",
        "            gs.append(g)\n",
        "\n",
        "        # Backward pass.\n",
        "        grad_g = self.cost_derivative(gs[-1], y)\n",
        "        grads_w = []\n",
        "        grads_b = []\n",
        "\n",
        "        delta = grad_g * sigmoid_prime(fs[-1])\n",
        "        grads_b.append(np.mean(delta, axis=0))\n",
        "        grads_w.append(delta.T @ gs[-2])\n",
        "\n",
        "        for l in range(2, self.num_layers + 1):\n",
        "            sp = sigmoid_prime(fs[-l])\n",
        "            delta = (delta @ self.weights[-l + 1]) * sp\n",
        "            grads_b.append(np.mean(delta, axis=0))\n",
        "            grads_w.append(delta.T @ gs[-l - 1])\n",
        "\n",
        "        grads_w.reverse()\n",
        "        grads_b.reverse()\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"grad_b {grad_b.shape=} != b {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"grad_w {grad_w.shape=} != w {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "\n",
        "\n",
        "    def cost_derivative(self, a: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
        "        \"\"\"\n",
        "        Gradient of loss (MSE) over output activations.\n",
        "\n",
        "        Args:\n",
        "        - a: output activations, shape (B, N^last).\n",
        "        - y: target values (one-hot encoded labels), shape (B, N^last).\n",
        "\n",
        "        Returns gradients, shape (B, N^last).\n",
        "        \"\"\"\n",
        "        assert a.shape == y.shape, f\"Shape mismatch: {a.shape=} but {y.shape=}\"\n",
        "        B, N_last = a.shape\n",
        "        return (2 / (B * N_last)) * (a - y.astype(np.float64))\n",
        "\n",
        "    def evaluate(self, x_test_data: FloatNDArray, y_test_data: FloatNDArray) -> np.float64:\n",
        "        \"\"\"\n",
        "        Compute accuracy: the ratio of correct answers for test_data.\n",
        "\n",
        "        Args:\n",
        "        - x_test_data: shape (B, N^0).\n",
        "        - y_test_data: shape (B, N^last).\n",
        "        \"\"\"\n",
        "        predictions = np.argmax(self.feedforward(x_test_data), axis=1)\n",
        "        targets = np.argmax(y_test_data, axis=1)\n",
        "        return np.mean(predictions == targets)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        training_data: tuple[FloatNDArray, FloatNDArray],\n",
        "        test_data: tuple[FloatNDArray, FloatNDArray] | None = None,\n",
        "        epochs: int = 2,\n",
        "        mini_batch_size: int = 100,\n",
        "        learning_rate: float = 1.0\n",
        "    ) -> np.float64:\n",
        "        x_train, y_train = training_data\n",
        "        progress_bar = tqdm(range(epochs), desc=\"Epoch\")\n",
        "        for epoch in progress_bar:\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                i_begin = i * mini_batch_size\n",
        "                i_end = (i + 1) * mini_batch_size\n",
        "                self.learning_step(x_train[i_begin:i_end], y_train[i_begin:i_end], learning_rate)\n",
        "            if test_data:\n",
        "                x_test, y_test = test_data\n",
        "                accuracy = self.evaluate(x_test, y_test)\n",
        "                progress_bar.set_postfix_str(f\"Test accuracy: {accuracy * 100:.2f} %\")\n",
        "\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "            return self.evaluate(x_test, y_test)\n",
        "        else:\n",
        "            return np.float64(-1)\n",
        "\n",
        "model_results.evaluate_model(model_name=\"Baseline\", model_constructor=Network, n_trainings=3)\n",
        "model_results.display_results()"
      ],
      "metadata": {
        "id": "tnBAMfMP6IRJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "ffea2be3-0ccd-46db-e0fe-afd4883c6bfe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:12<00:00,  1.20s/it, Test accuracy: 91.14 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Test accuracy: 91.16 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.10s/it, Test accuracy: 91.23 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:09<00:00,  1.04it/s, Test accuracy: 94.90 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:12<00:00,  1.28s/it, Test accuracy: 95.01 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Test accuracy: 95.06 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it, Test accuracy: 8.92 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Test accuracy: 29.56 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it, Test accuracy: 13.40 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a97c0bf0ec0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_19147\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_19147_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_19147_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_19147_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_19147_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_19147_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_19147_row0_col2\" class=\"data row0 col2\" >94.9% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_19147_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
              "      <td id=\"T_19147_row1_col1\" class=\"data row1 col1\" >1</td>\n",
              "      <td id=\"T_19147_row1_col2\" class=\"data row1 col2\" >91.1% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_19147_row2_col0\" class=\"data row2 col0\" >SoftMax</td>\n",
              "      <td id=\"T_19147_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_19147_row2_col2\" class=\"data row2 col2\" >28.7% ± 18.9 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_19147_row3_col0\" class=\"data row3 col0\" >Baseline</td>\n",
              "      <td id=\"T_19147_row3_col1\" class=\"data row3 col1\" >1e+02</td>\n",
              "      <td id=\"T_19147_row3_col2\" class=\"data row3 col2\" >13.5% ± 7.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_19147_row4_col0\" class=\"data row4 col0\" >SoftMax</td>\n",
              "      <td id=\"T_19147_row4_col1\" class=\"data row4 col1\" >1e+01</td>\n",
              "      <td id=\"T_19147_row4_col2\" class=\"data row4 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_19147_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n",
              "      <td id=\"T_19147_row5_col1\" class=\"data row5 col1\" >1e+02</td>\n",
              "      <td id=\"T_19147_row5_col2\" class=\"data row5 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpZIY72SXywD"
      },
      "source": [
        "## Task 1: softmax & cross-entropy loss\n",
        "Use softmax instead of coordinate-wise sigmoid and use negative-log-loss instead of MSE. Test to see if this improves convergence.   \n",
        "\n",
        "Hints:\n",
        "* When implementing backprop it's easier to consider these two functions as a single block, skipping the computation of the gradient over the softmax values, and going directly to gradients over logits (last pre-activations).\n",
        "* Softmax is only used after the last layer; previous layers (and their grad computations) can be unchanged.\n",
        "* Remember to update the forward pass in both places.\n",
        "* Loss for a mini-batch is the mean of losses for each dataitem in it, by convention.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "77iGbiSDXywD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "8cc00871-34f2-43cb-d2a6-e619a2fb19ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-3898644277.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.02s/it, Test accuracy: 59.58 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.15it/s, Test accuracy: 20.41 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 39.14 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-1495327556.py:49: RuntimeWarning: overflow encountered in matmul\n",
            "  delta = (delta @ self.weights[-l + 1]) * sp\n",
            "/tmp/ipython-input-1495327556.py:49: RuntimeWarning: invalid value encountered in multiply\n",
            "  delta = (delta @ self.weights[-l + 1]) * sp\n",
            "Epoch: 100%|██████████| 10/10 [00:12<00:00,  1.27s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.02s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.25it/s, Test accuracy: 9.80 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.06s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.28it/s, Test accuracy: 9.80 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a97bf633e90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_d96a6\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_d96a6_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_d96a6_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_d96a6_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_d96a6_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_d96a6_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_d96a6_row0_col2\" class=\"data row0 col2\" >94.9% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_d96a6_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
              "      <td id=\"T_d96a6_row1_col1\" class=\"data row1 col1\" >1</td>\n",
              "      <td id=\"T_d96a6_row1_col2\" class=\"data row1 col2\" >91.1% ± 0.2 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_d96a6_row2_col0\" class=\"data row2 col0\" >SoftMax</td>\n",
              "      <td id=\"T_d96a6_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_d96a6_row2_col2\" class=\"data row2 col2\" >24.8% ± 18.8 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_d96a6_row3_col0\" class=\"data row3 col0\" >SoftMax</td>\n",
              "      <td id=\"T_d96a6_row3_col1\" class=\"data row3 col1\" >1e+01</td>\n",
              "      <td id=\"T_d96a6_row3_col2\" class=\"data row3 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_d96a6_row4_col0\" class=\"data row4 col0\" >SoftMax</td>\n",
              "      <td id=\"T_d96a6_row4_col1\" class=\"data row4 col1\" >1e+02</td>\n",
              "      <td id=\"T_d96a6_row4_col2\" class=\"data row4 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_d96a6_row5_col0\" class=\"data row5 col0\" >Baseline</td>\n",
              "      <td id=\"T_d96a6_row5_col1\" class=\"data row5 col1\" >1e+02</td>\n",
              "      <td id=\"T_d96a6_row5_col2\" class=\"data row5 col2\" >9.8% ± 0.6 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from re import X\n",
        "class Task1(Network):\n",
        "    def __init__(self, sizes: Sequence[int]):\n",
        "        super().__init__(sizes=sizes)\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        g = x\n",
        "        for w, b in zip(self.weights[:-1], self.biases[:-1], strict=True):\n",
        "            f = g @ w.T + b\n",
        "            g = sigmoid(f)\n",
        "\n",
        "        # last layer\n",
        "        f = g @ self.weights[-1].T + self.biases[-1]\n",
        "        g = stable_softmax(f)\n",
        "        return g\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        # Forward pass.\n",
        "        fs, gs = [], [x]\n",
        "        for w, b in zip(self.weights, self.biases, strict=True):\n",
        "            f = gs[-1] @ w.T + b\n",
        "            fs.append(f)\n",
        "            # last layer\n",
        "            if w is self.weights[-1]:\n",
        "                g = stable_softmax(f)\n",
        "            else:\n",
        "                g = sigmoid(f)\n",
        "            gs.append(g)\n",
        "\n",
        "\n",
        "        grads_w = [None] * len(self.weights)\n",
        "        grads_b = [None] * len(self.biases)\n",
        "\n",
        "        # delta^L = a^L - y  (dla softmax + cross-entropy)\n",
        "        delta = (g - y) / B  # dzielimy przez batch size już tutaj\n",
        "\n",
        "        grads_w[-1] = delta.T @ gs[-2]  # (N^L, N^{L-1})\n",
        "        grads_b[-1] = np.sum(delta, axis=0)  # (N^L,)\n",
        "\n",
        "        # wcześniejsze warstwy: standardowy backprop z sigmoidem\n",
        "        for l in range(2, len(self.sizes)):\n",
        "            z = fs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = (delta @ self.weights[-l + 1]) * sp\n",
        "\n",
        "            grads_w[-l] = delta.T @ gs[-l - 1]\n",
        "            grads_b[-l] = np.sum(delta, axis=0)\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "    def cost_derivative(self, a: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
        "      \"\"\"\n",
        "      Gradient of the cross-entropy loss over output activations (a).\n",
        "      For softmax followed by cross-entropy, this is simply a - y.\n",
        "\n",
        "      Args:\n",
        "      - a: output activations (after softmax), shape (B, N^last).\n",
        "      - y: target values (one-hot encoded labels), shape (B, N^last).\n",
        "\n",
        "      Returns gradients, shape (B, N^last).\n",
        "      \"\"\"\n",
        "      assert a.shape == y.shape, f\"Shape mismatch: {a.shape=} but {y.shape=}\"\n",
        "      return (a - y.astype(np.float64))\n",
        "\n",
        "\n",
        "model_results.evaluate_model(model_name=\"SoftMax\", model_constructor=Task1, n_trainings=3)\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Task1(Network):\n",
        "    def __init__(self, sizes: Sequence[int]):\n",
        "        super().__init__(sizes=sizes)\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        g = x\n",
        "        # wszystkie warstwy ukryte — sigmoid\n",
        "        for W, b in zip(self.weights[:-1], self.biases[:-1]):\n",
        "          f = g @ W.T + b\n",
        "          g = sigmoid(f)\n",
        "        # ostatnia warstwa — softmax\n",
        "        f = g @ self.weights[-1].T + self.biases[-1]\n",
        "        g = stable_softmax(f, axis=1)\n",
        "        return g\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        # Forward pass.\n",
        "        # Activations (including input) of shapes (B, N^0), (B, N^1), ..., (B, N^last).\n",
        "        gs: list[FloatNDArray] = [x]\n",
        "        zs: list[FloatNDArray] = []   # sumy: z^1, ..., z^L\n",
        "\n",
        "        g = x\n",
        "        for W, b in zip(self.weights[:-1], self.biases[:-1]):\n",
        "            z = g @ W.T + b\n",
        "            zs.append(z)\n",
        "            g = sigmoid(z)\n",
        "            gs.append(g)\n",
        "\n",
        "        # ostatnia warstwa — softmax\n",
        "        zL = g @ self.weights[-1].T + self.biases[-1]\n",
        "        zs.append(zL)\n",
        "        gL = stable_softmax(zL, axis=1)\n",
        "        gs.append(gL)\n",
        "\n",
        "        # Backward pass.\n",
        "        grads_w = [None] * len(self.weights)\n",
        "        grads_b = [None] * len(self.biases)\n",
        "\n",
        "        # delta^L = a^L - y  (dla softmax + cross-entropy)\n",
        "        delta = (gL - y) # Gradient of the loss with respect to the output of the softmax layer\n",
        "\n",
        "        # Gradients for the last layer\n",
        "        grads_w[-1] = gs[-2].T @ delta  # (N^{L-1}, B) @ (B, N^L) -> (N^{L-1}, N^L) - This should be transposed later\n",
        "        grads_b[-1] = np.sum(delta, axis=0)  # (N^L,)\n",
        "\n",
        "        # wsteczna propagacja przez warstwy ukryte (sigmoid)\n",
        "        for l in range(2, len(self.sizes)):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = (delta @ self.weights[-l + 1]) * sp\n",
        "\n",
        "            grads_w[-l] = gs[-l-1].T @ delta # (N^{l-1}, B) @ (B, N^l) -> (N^{l-1}, N^l) - This should be transposed later\n",
        "            grads_b[-l] = np.sum(delta, axis=0)\n",
        "\n",
        "        # Transpose weight gradients to match the shape (N^i, N^{i-1})\n",
        "        grads_w = [grad_w.T for grad_w in grads_w]\n",
        "\n",
        "        # Divide gradients by batch size\n",
        "        grads_w = [grad_w / B for grad_w in grads_w]\n",
        "        grads_b = [grad_b / B for grad_b in grads_b]\n",
        "\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "    def cost_derivative(self, a: FloatNDArray, y: FloatNDArray) -> FloatNDArray:\n",
        "      \"\"\"\n",
        "      Gradient of the cross-entropy loss over output activations (a).\n",
        "      For softmax followed by cross-entropy, this is simply a - y.\n",
        "\n",
        "      Args:\n",
        "      - a: output activations (after softmax), shape (B, N^last).\n",
        "      - y: target values (one-hot encoded labels), shape (B, N^last).\n",
        "\n",
        "      Returns gradients, shape (B, N^last).\n",
        "      \"\"\"\n",
        "      assert a.shape == y.shape, f\"Shape mismatch: {a.shape=} but {y.shape=}\"\n",
        "      return (a - y.astype(np.float64))\n",
        "\n",
        "\n",
        "model_results.evaluate_model(model_name=\"SoftMax\", model_constructor=Task1, n_trainings=3)\n",
        "model_results.display_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "8-wEkrLzddUY",
        "outputId": "73f3e15c-0564-43ca-b545-afd5b53149aa"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-3898644277.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n",
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.13it/s, Test accuracy: 29.85 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.19s/it, Test accuracy: 39.37 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it, Test accuracy: 81.31 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-3485453322.py:55: RuntimeWarning: overflow encountered in matmul\n",
            "  delta = (delta @ self.weights[-l + 1]) * sp\n",
            "/tmp/ipython-input-3485453322.py:55: RuntimeWarning: invalid value encountered in multiply\n",
            "  delta = (delta @ self.weights[-l + 1]) * sp\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.22it/s, Test accuracy: 9.80 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.20s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it, Test accuracy: 9.80 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a97bf633d40>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_74592\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_74592_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_74592_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_74592_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_74592_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_74592_row0_col2\" class=\"data row0 col2\" >94.9% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row1_col0\" class=\"data row1 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_74592_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n",
              "      <td id=\"T_74592_row1_col2\" class=\"data row1 col2\" >94.4% ± 0.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row2_col0\" class=\"data row2 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_74592_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_74592_row2_col2\" class=\"data row2 col2\" >91.6% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row3_col0\" class=\"data row3 col0\" >Baseline</td>\n",
              "      <td id=\"T_74592_row3_col1\" class=\"data row3 col1\" >1</td>\n",
              "      <td id=\"T_74592_row3_col2\" class=\"data row3 col2\" >91.1% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row4_col0\" class=\"data row4 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_74592_row4_col1\" class=\"data row4 col1\" >1e+02</td>\n",
              "      <td id=\"T_74592_row4_col2\" class=\"data row4 col2\" >89.3% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n",
              "      <td id=\"T_74592_row5_col1\" class=\"data row5 col1\" >1</td>\n",
              "      <td id=\"T_74592_row5_col2\" class=\"data row5 col2\" >33.5% ± 18.5 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row6_col0\" class=\"data row6 col0\" >Baseline</td>\n",
              "      <td id=\"T_74592_row6_col1\" class=\"data row6 col1\" >1e+02</td>\n",
              "      <td id=\"T_74592_row6_col2\" class=\"data row6 col2\" >13.5% ± 7.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row7_col0\" class=\"data row7 col0\" >Softmax&L2&Momentum(l2_factor=1e-06,momentum=0.1)</td>\n",
              "      <td id=\"T_74592_row7_col1\" class=\"data row7 col1\" >2</td>\n",
              "      <td id=\"T_74592_row7_col2\" class=\"data row7 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row8_col0\" class=\"data row8 col0\" >SoftMax</td>\n",
              "      <td id=\"T_74592_row8_col1\" class=\"data row8 col1\" >1e+01</td>\n",
              "      <td id=\"T_74592_row8_col2\" class=\"data row8 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_74592_row9_col0\" class=\"data row9 col0\" >SoftMax</td>\n",
              "      <td id=\"T_74592_row9_col1\" class=\"data row9 col1\" >1e+02</td>\n",
              "      <td id=\"T_74592_row9_col2\" class=\"data row9 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Task1(Network):\n",
        "    def __init__(self, sizes: Sequence[int]):\n",
        "        super().__init__(sizes=sizes)\n",
        "\n",
        "    def feedforward(self, x: FloatNDArray) -> FloatNDArray:\n",
        "        g = x\n",
        "        # wszystkie warstwy ukryte — sigmoid\n",
        "        for W, b in zip(self.weights[:-1], self.biases[:-1]):\n",
        "          g = sigmoid(g @ W.T + b)\n",
        "        # ostatnia warstwa — softmax\n",
        "        g = stable_softmax(g @ self.weights[-1].T + self.biases[-1], axis=1)\n",
        "        return g\n",
        "\n",
        "    def backprop(\n",
        "        self, x: FloatNDArray, y: FloatNDArray\n",
        "    ) -> tuple[list[FloatNDArray], list[FloatNDArray]]:\n",
        "        B, N0 = x.shape\n",
        "        assert N0 == self.sizes[0]\n",
        "\n",
        "        # Forward pass.\n",
        "        # Activations (including input) of shapes (B, N^0), (B, N^1), ..., (B, N^last).\n",
        "        gs: list[FloatNDArray] = [x]\n",
        "        zs: list[FloatNDArray] = []   # sumy: z^1, ..., z^L\n",
        "\n",
        "        g = x\n",
        "        for W, b in zip(self.weights[:-1], self.biases[:-1]):\n",
        "            z = g @ W.T + b\n",
        "            zs.append(z)\n",
        "            g = sigmoid(z)\n",
        "            gs.append(g)\n",
        "\n",
        "        # ostatnia warstwa — softmax\n",
        "        zL = g @ self.weights[-1].T + self.biases[-1]\n",
        "        zs.append(zL)\n",
        "        gL = stable_softmax(zL, axis=1)\n",
        "        gs.append(gL)\n",
        "\n",
        "        # Backward pass.\n",
        "        ## TODO\n",
        "        ###{\n",
        "        grads_w = [None] * len(self.weights)\n",
        "        grads_b = [None] * len(self.biases)\n",
        "\n",
        "        # delta^L = a^L - y  (dla softmax + cross-entropy)\n",
        "        delta = (gL - y) / B  # dzielimy przez batch size już tutaj\n",
        "\n",
        "        grads_w[-1] = delta.T @ gs[-2]  # (N^L, N^{L-1})\n",
        "        grads_b[-1] = np.sum(delta, axis=0)  # (N^L,)\n",
        "\n",
        "        # wcześniejsze warstwy: standardowy backprop z sigmoidem\n",
        "        for l in range(2, len(self.sizes)):\n",
        "            z = zs[-l]\n",
        "            sp = sigmoid_prime(z)\n",
        "            delta = (delta @ self.weights[-l + 1]) * sp\n",
        "\n",
        "            grads_w[-l] = delta.T @ gs[-l - 1]\n",
        "            grads_b[-l] = np.sum(delta, axis=0)\n",
        "\n",
        "        for grad_b, b in zip(grads_b, self.biases, strict=True):\n",
        "            assert grad_b.shape == b.shape, f\"Shape mismatch: {grad_b.shape=} but {b.shape=}\"\n",
        "        for grad_w, w in zip(grads_w, self.weights, strict=True):\n",
        "            assert grad_w.shape == w.shape, f\"Shape mismatch: {grad_w.shape=} but {w.shape=}\"\n",
        "\n",
        "        return grads_w, grads_b\n",
        "\n",
        "\n",
        "model_results.evaluate_model(model_name=\"SoftMax\", model_constructor=Task1, n_trainings=3)\n",
        "model_results.display_results()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "u2KWM7xYe96o",
        "outputId": "df03be69-cc6d-4776-e107-eb099af8bee8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-3898644277.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.09s/it, Test accuracy: 48.40 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.18it/s, Test accuracy: 31.38 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.03s/it, Test accuracy: 20.53 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-3742605389.py:54: RuntimeWarning: overflow encountered in matmul\n",
            "  delta = (delta @ self.weights[-l + 1]) * sp\n",
            "/tmp/ipython-input-3742605389.py:54: RuntimeWarning: invalid value encountered in multiply\n",
            "  delta = (delta @ self.weights[-l + 1]) * sp\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.03s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.02s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:08<00:00,  1.23it/s, Test accuracy: 9.80 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.05s/it, Test accuracy: 9.80 %]\n",
            "Epoch: 100%|██████████| 10/10 [00:07<00:00,  1.26it/s, Test accuracy: 9.80 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a97bf740c50>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_ef080\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_ef080_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_ef080_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_ef080_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_ef080_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_ef080_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_ef080_row0_col2\" class=\"data row0 col2\" >94.9% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_ef080_row1_col0\" class=\"data row1 col0\" >Baseline</td>\n",
              "      <td id=\"T_ef080_row1_col1\" class=\"data row1 col1\" >1</td>\n",
              "      <td id=\"T_ef080_row1_col2\" class=\"data row1 col2\" >91.1% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_ef080_row2_col0\" class=\"data row2 col0\" >SoftMax</td>\n",
              "      <td id=\"T_ef080_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_ef080_row2_col2\" class=\"data row2 col2\" >30.2% ± 15.6 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_ef080_row3_col0\" class=\"data row3 col0\" >Baseline</td>\n",
              "      <td id=\"T_ef080_row3_col1\" class=\"data row3 col1\" >1e+02</td>\n",
              "      <td id=\"T_ef080_row3_col2\" class=\"data row3 col2\" >13.5% ± 7.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_ef080_row4_col0\" class=\"data row4 col0\" >SoftMax</td>\n",
              "      <td id=\"T_ef080_row4_col1\" class=\"data row4 col1\" >1e+01</td>\n",
              "      <td id=\"T_ef080_row4_col2\" class=\"data row4 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_ef080_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n",
              "      <td id=\"T_ef080_row5_col1\" class=\"data row5 col1\" >1e+02</td>\n",
              "      <td id=\"T_ef080_row5_col2\" class=\"data row5 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvIk4RxTXywD"
      },
      "source": [
        "## Task 2: L2-regularization and momentum\n",
        "Implement L2-regularization and add momentum to the SGD algorithm. Play with different amounts of regularization and momentum. See if this improves accuracy/convergence.  \n",
        "A few notes:\n",
        "* do not regularize the biases\n",
        "* you can see an example pseudocode here [pytorch.org/docs/stable/generated/torch.optim.SGD.html](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "s3-03midXywD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "c50251f5-8d19-49b5-8110-414d15c2cf4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 1 random trainings with with lr = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:12<00:00,  1.26s/it, Test accuracy: 91.60 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 1 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:12<00:00,  1.26s/it, Test accuracy: 94.20 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 1 random trainings with with lr = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:12<00:00,  1.24s/it, Test accuracy: 89.26 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a97bf517ad0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_3c9f7\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_3c9f7_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_3c9f7_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_3c9f7_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_3c9f7_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_3c9f7_row0_col2\" class=\"data row0 col2\" >94.9% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row1_col0\" class=\"data row1 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_3c9f7_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n",
              "      <td id=\"T_3c9f7_row1_col2\" class=\"data row1 col2\" >94.4% ± 0.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row2_col0\" class=\"data row2 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_3c9f7_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_3c9f7_row2_col2\" class=\"data row2 col2\" >91.6% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row3_col0\" class=\"data row3 col0\" >Baseline</td>\n",
              "      <td id=\"T_3c9f7_row3_col1\" class=\"data row3 col1\" >1</td>\n",
              "      <td id=\"T_3c9f7_row3_col2\" class=\"data row3 col2\" >91.1% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row4_col0\" class=\"data row4 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_3c9f7_row4_col1\" class=\"data row4 col1\" >1e+02</td>\n",
              "      <td id=\"T_3c9f7_row4_col2\" class=\"data row4 col2\" >89.3% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n",
              "      <td id=\"T_3c9f7_row5_col1\" class=\"data row5 col1\" >1</td>\n",
              "      <td id=\"T_3c9f7_row5_col2\" class=\"data row5 col2\" >30.2% ± 15.6 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row6_col0\" class=\"data row6 col0\" >Baseline</td>\n",
              "      <td id=\"T_3c9f7_row6_col1\" class=\"data row6 col1\" >1e+02</td>\n",
              "      <td id=\"T_3c9f7_row6_col2\" class=\"data row6 col2\" >13.5% ± 7.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row7_col0\" class=\"data row7 col0\" >Softmax&L2&Momentum(l2_factor=1e-06,momentum=0.1)</td>\n",
              "      <td id=\"T_3c9f7_row7_col1\" class=\"data row7 col1\" >2</td>\n",
              "      <td id=\"T_3c9f7_row7_col2\" class=\"data row7 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row8_col0\" class=\"data row8 col0\" >SoftMax</td>\n",
              "      <td id=\"T_3c9f7_row8_col1\" class=\"data row8 col1\" >1e+01</td>\n",
              "      <td id=\"T_3c9f7_row8_col2\" class=\"data row8 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_3c9f7_row9_col0\" class=\"data row9 col0\" >SoftMax</td>\n",
              "      <td id=\"T_3c9f7_row9_col1\" class=\"data row9 col1\" >1e+02</td>\n",
              "      <td id=\"T_3c9f7_row9_col2\" class=\"data row9 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "class Task2(Network):\n",
        "    def __init__(\n",
        "        self, sizes: Sequence[int], l2_factor: float = 1e-5, momentum: float = 0.2\n",
        "    ):\n",
        "        super().__init__(sizes=sizes)\n",
        "        ## TODO\n",
        "        ####{\n",
        "        self.l2_factor = l2_factor\n",
        "        self.momentum = momentum\n",
        "        self.velocity_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.velocity_b = [np.zeros_like(b) for b in self.biases]\n",
        "        ###}\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update parameters with one mini-batch step of backprop and gradient descent (with momentum and L2-regularization).\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        ## TODO\n",
        "        ###{\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        # Add L2 regularization gradient to weights (not biases)\n",
        "        grads_w = [grad_w + self.l2_factor *2* w for grad_w, w in zip(grads_w, self.weights)]\n",
        "\n",
        "        # Update velocity\n",
        "        self.velocity_w = [self.momentum * v_w + learning_rate * grad_w for v_w, grad_w in zip(self.velocity_w, grads_w)]\n",
        "        self.velocity_b = [self.momentum * v_b + learning_rate * grad_b for v_b, grad_b in zip(self.velocity_b, grads_b)]\n",
        "\n",
        "        # Update weights and biases using velocity\n",
        "        self.weights = [w - v_w for w, v_w in zip(self.weights, self.velocity_w)]\n",
        "        self.biases = [b - v_b for b, v_b in zip(self.biases, self.velocity_b)]\n",
        "\n",
        "        ###}\n",
        "\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"L2&Momentum\",\n",
        "    model_constructor=Task2,\n",
        "    learning_rates=[1, 10.0, 100],\n",
        "    n_trainings=1,\n",
        "    l2_factor=1e-5,\n",
        "    momentum=0.2\n",
        ")\n",
        "model_results.display_results()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Task1And2(Task2, Task1):\n",
        "    # A somewhat hacky but short way to mix Task1 and Task2.\n",
        "    # You could also just replace the superclass of Task2 to be Task1.\n",
        "    pass\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"Softmax&L2&Momentum\",\n",
        "    model_constructor=Task1And2,\n",
        "    learning_rates=[2.0],\n",
        "    n_trainings=3,\n",
        "    l2_factor=1e-6,\n",
        "    momentum=0.1\n",
        ")\n",
        "model_results.display_results()"
      ],
      "metadata": {
        "id": "nnBGG1xo0F34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "840bbd39-87c7-4f60-df3c-eed3d1581fce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipython-input-3898644277.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n",
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it, Test accuracy: 9.82 %]\n",
            "Epoch:  50%|█████     | 5/10 [00:05<00:05,  1.05s/it, Test accuracy: 9.82 %]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2913980918.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model_results.evaluate_model(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Softmax&L2&Momentum\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTask1And2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3806505274.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, model_name, model_constructor, layers, learning_rates, n_trainings, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trainings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 accuracy = network.train(\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4034840158.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, test_data, epochs, mini_batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mi_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mi_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2421044208.py\u001b[0m in \u001b[0;36mlearning_step\u001b[0;34m(self, x_mini_batch, y_mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m## TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m###{\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgrads_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Add L2 regularization gradient to weights (not biases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3742605389.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mgrads_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mgrads_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgrad_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36m_sum_dispatcher\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2250\u001b[0;31m def _sum_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n\u001b[0m\u001b[1;32m   2251\u001b[0m                     initial=None, where=None):\n\u001b[1;32m   2252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6nLauKUXywE"
      },
      "source": [
        "## Task 3 (optional)\n",
        "Implement more variations of SGD:\n",
        "* AdamW (probably the most popular choice) or Adagrad,\n",
        "* dropout\n",
        "* some simple data augmentations (e.g. tiny rotations/shifts etc.).\n",
        "\n",
        "Again, test to see how these changes improve accuracy/convergence.  \n",
        "\n",
        "Quick reminders:\n",
        "* for AdamW, check the official [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)'s pseudocode or the original paper: [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101).\n",
        "* for AdaGrad, check the Appendix of this notebook.\n",
        "* for dropout: during training only, zero-out each activation in the considered layer with probability $p$, and multiplying other activations by $\\frac{1}{1-p}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X3hRIizXywE"
      },
      "outputs": [],
      "source": [
        "# Place for remaining parts of task 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HCbwiW2XywE"
      },
      "source": [
        "## Task 4\n",
        "Try adding extra layers to the network. Again, test how the changes you introduced affect accuracy/convergence and what learning rates work.\n",
        "\n",
        "As a start, you can try this slightly larger architecture: [784,100,30,10]  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92OPX1uCXywE"
      },
      "outputs": [],
      "source": [
        "## TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xl3A1WSXywE"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRX8ith-XywF"
      },
      "source": [
        "## Adagrad (simplified version)\n",
        "\n",
        "Let $p_1, \\ldots, p_n$ be all parameters in our model (weights and biases).  \n",
        "For parameter $p_i$ we maintain a variable $G_i$ (can be set to $0$ initially).\n",
        "Let $\\mathcal{L}$ be our loss without L2.   \n",
        "We update $G_i$ and $p_i$ each training step as follows:  \n",
        "$$\n",
        "G_i = G_i +  \\left(\\frac{\\partial \\mathcal{L}}{\\partial p_i}\\right)^2\\\\\n",
        "p_i = p_i - \\frac{\\eta}{\\sqrt{\\left(G_i + \\epsilon\\right)}}\\frac{\\partial \\mathcal{L}}{\\partial p_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfBEYR4rYXmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d74f412e"
      },
      "source": [
        "# Task\n",
        "Implement L2 regularization and momentum in the `Task2` class by adding parameters for L2 factor and momentum to the `__init__` method, modifying the `learning_step` method to include the gradient of the L2 regularization term and incorporate momentum, and then evaluate the model using `evaluate_model`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42164617"
      },
      "source": [
        "## Initialize parameters\n",
        "\n",
        "### Subtask:\n",
        "Add parameters for L2 factor and momentum to the `__init__` method of the `Task2` class and initialize variables to store the velocity for momentum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27fd5bcc"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to modify the `__init__` method of the `Task2` class to include L2 factor and momentum parameters and initialize the velocity variables for momentum. This can be done by adding parameters to the `__init__` method and initializing the `velocity_w` and `velocity_b` attributes with zeros.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "369d4ec1",
        "outputId": "92669369-55bd-4d91-b330-1b1104ffec3e"
      },
      "source": [
        "class Task2(Network):\n",
        "    def __init__(\n",
        "        self, sizes: Sequence[int], l2_factor: float = 1e-5, momentum: float = 0.2\n",
        "    ):\n",
        "        super().__init__(sizes=sizes)\n",
        "        ## TODO\n",
        "        ####{\n",
        "        self.l2_factor = l2_factor\n",
        "        self.momentum = momentum\n",
        "        self.velocity_w = [np.zeros_like(w) for w in self.weights]\n",
        "        self.velocity_b = [np.zeros_like(b) for b in self.biases]\n",
        "        ###}\n",
        "\n",
        "    def learning_step(self, x_mini_batch: FloatNDArray, y_mini_batch: FloatNDArray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Update parameters with one mini-batch step of backprop and gradient descent (with momentum and L2-regularization).\n",
        "\n",
        "        Args:\n",
        "        - x_mini_batch: shape (B, N^0) where B is mini_batch_size.\n",
        "        - y_mini_batch: shape (B, N^last).\n",
        "        - learning_rate.\n",
        "        \"\"\"\n",
        "        ## TODO\n",
        "        ###{\n",
        "        grads_w, grads_b = self.backprop(x_mini_batch, y_mini_batch)\n",
        "\n",
        "        # Add L2 regularization gradient to weights (not biases)\n",
        "        grads_w = [grad_w + self.l2_factor *2* w for grad_w, w in zip(grads_w, self.weights)]\n",
        "\n",
        "        # Update velocity\n",
        "        self.velocity_w = [self.momentum * v_w + learning_rate * grad_w for v_w, grad_w in zip(self.velocity_w, grads_w)]\n",
        "        self.velocity_b = [self.momentum * v_b + learning_rate * grad_b for v_b, grad_b in zip(self.velocity_b, grads_b)]\n",
        "\n",
        "        # Update weights and biases using velocity\n",
        "        self.weights = [w - v_w for w, v_w in zip(self.weights, self.velocity_w)]\n",
        "        self.biases = [b - v_b for b, v_b in zip(self.biases, self.velocity_b)]\n",
        "\n",
        "        ###}\n",
        "\n",
        "\n",
        "model_results.evaluate_model(\n",
        "    model_name=f\"L2&Momentum\",\n",
        "    model_constructor=Task2,\n",
        "    learning_rates=[1, 10.0, 100],\n",
        "    n_trainings=1,\n",
        "    l2_factor=1e-5,\n",
        "    momentum=0.2\n",
        ")\n",
        "model_results.display_results()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 1 random trainings with with lr = 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:11<00:00,  1.15s/it, Test accuracy: 91.53 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 1 random trainings with with lr = 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:10<00:00,  1.10s/it, Test accuracy: 94.25 %]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 1 random trainings with with lr = 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 10/10 [00:13<00:00,  1.37s/it, Test accuracy: 89.44 %]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7a97bf51aea0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_f1ea4\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_f1ea4_level0_col0\" class=\"col_heading level0 col0\" >model</th>\n",
              "      <th id=\"T_f1ea4_level0_col1\" class=\"col_heading level0 col1\" >lr</th>\n",
              "      <th id=\"T_f1ea4_level0_col2\" class=\"col_heading level0 col2\" >accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row0_col0\" class=\"data row0 col0\" >Baseline</td>\n",
              "      <td id=\"T_f1ea4_row0_col1\" class=\"data row0 col1\" >1e+01</td>\n",
              "      <td id=\"T_f1ea4_row0_col2\" class=\"data row0 col2\" >94.9% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row1_col0\" class=\"data row1 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_f1ea4_row1_col1\" class=\"data row1 col1\" >1e+01</td>\n",
              "      <td id=\"T_f1ea4_row1_col2\" class=\"data row1 col2\" >94.5% ± 0.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row2_col0\" class=\"data row2 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_f1ea4_row2_col1\" class=\"data row2 col1\" >1</td>\n",
              "      <td id=\"T_f1ea4_row2_col2\" class=\"data row2 col2\" >91.5% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row3_col0\" class=\"data row3 col0\" >Baseline</td>\n",
              "      <td id=\"T_f1ea4_row3_col1\" class=\"data row3 col1\" >1</td>\n",
              "      <td id=\"T_f1ea4_row3_col2\" class=\"data row3 col2\" >91.1% ± 0.1 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row4_col0\" class=\"data row4 col0\" >L2&Momentum(l2_factor=1e-05,momentum=0.2)</td>\n",
              "      <td id=\"T_f1ea4_row4_col1\" class=\"data row4 col1\" >1e+02</td>\n",
              "      <td id=\"T_f1ea4_row4_col2\" class=\"data row4 col2\" >89.4% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row5_col0\" class=\"data row5 col0\" >SoftMax</td>\n",
              "      <td id=\"T_f1ea4_row5_col1\" class=\"data row5 col1\" >1</td>\n",
              "      <td id=\"T_f1ea4_row5_col2\" class=\"data row5 col2\" >30.2% ± 15.6 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row6_col0\" class=\"data row6 col0\" >Baseline</td>\n",
              "      <td id=\"T_f1ea4_row6_col1\" class=\"data row6 col1\" >1e+02</td>\n",
              "      <td id=\"T_f1ea4_row6_col2\" class=\"data row6 col2\" >13.5% ± 7.3 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row7_col0\" class=\"data row7 col0\" >SoftMax</td>\n",
              "      <td id=\"T_f1ea4_row7_col1\" class=\"data row7 col1\" >1e+01</td>\n",
              "      <td id=\"T_f1ea4_row7_col2\" class=\"data row7 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_f1ea4_row8_col0\" class=\"data row8 col0\" >SoftMax</td>\n",
              "      <td id=\"T_f1ea4_row8_col1\" class=\"data row8 col1\" >1e+02</td>\n",
              "      <td id=\"T_f1ea4_row8_col2\" class=\"data row8 col2\" >9.8% ± 0.0 p.p.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42aa55f7"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The `Task2` class was successfully modified to include `l2_factor` and `momentum` parameters in the `__init__` method, along with initializing `velocity_w` and `velocity_b` for momentum.\n",
        "* The `learning_step` method was updated to incorporate the gradient of the L2 regularization term for weights and to use momentum for updating both weights and biases.\n",
        "* The model trained with L2 regularization (factor 1e-5) and momentum (0.2) achieved a test accuracy of 94.80%.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* The implementation of L2 regularization and momentum appears to be successful, as the model trained and evaluated correctly with these additions.\n",
        "* Further analysis could involve hyperparameter tuning of the L2 factor and momentum values to potentially improve model performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0057fcbc"
      },
      "source": [
        "# Task\n",
        "Implement a deeper network architecture and evaluate its performance compared to previous models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ac57f4"
      },
      "source": [
        "## Define a new network architecture\n",
        "\n",
        "### Subtask:\n",
        "Specify a new sequence of layer sizes that includes more layers than the baseline model (e.g., `[784, 100, 30, 10]`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7effebd"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the list `deeper_network_sizes` with the specified architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0090b9d"
      },
      "source": [
        "deeper_network_sizes = [784, 100, 30, 10]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9c47554"
      },
      "source": [
        "## Create a new network instance with the updated architecture\n",
        "\n",
        "### Subtask:\n",
        "Instantiate a new `Network` object (or `Task1` or `Task1And2` if you want to include softmax/cross-entropy and momentum/L2) using the new layer sizes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81c97094"
      },
      "source": [
        "**Reasoning**:\n",
        "Instantiate a new Task1And2 network with the deeper architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "036036a4"
      },
      "source": [
        "deeper_network = Task2(sizes=deeper_network_sizes, l2_factor=1e-6, momentum=0.1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "606b72cd"
      },
      "source": [
        "## Evaluate the new model\n",
        "\n",
        "### Subtask:\n",
        "Use the `evaluate_model` function to train and evaluate the network with the new architecture and different learning rates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ddcd82"
      },
      "source": [
        "**Reasoning**:\n",
        "Call the `model_results.evaluate_model` function to train and evaluate the network with the deeper architecture and different learning rates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ca4aa60e",
        "outputId": "21630e80-1d44-434f-cda9-8a987a17e4d9"
      },
      "source": [
        "model_results.evaluate_model(\n",
        "    model_name=\"Deeper Network (Softmax&L2&Momentum)\",\n",
        "    model_constructor=Task2,\n",
        "    layers=deeper_network_sizes,\n",
        "    learning_rates=[1.0, 10.0, 100.0],\n",
        "    n_trainings=3,\n",
        "    l2_factor=1e-6,\n",
        "    momentum=0.1\n",
        ")\n",
        "model_results.display_results()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking 3 random trainings with with lr = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  90%|█████████ | 9/10 [00:27<00:03,  3.08s/it, Test accuracy: 88.17 %]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1543398203.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_results.evaluate_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Deeper Network (Softmax&L2&Momentum)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeeper_network_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlearning_rates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3806505274.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self, model_name, model_constructor, layers, learning_rates, n_trainings, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_trainings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 accuracy = network.train(\n\u001b[0m\u001b[1;32m     68\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4034840158.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, test_data, epochs, mini_batch_size, learning_rate)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mi_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mi_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2421044208.py\u001b[0m in \u001b[0;36mlearning_step\u001b[0;34m(self, x_mini_batch, y_mini_batch, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m## TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m###{\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgrads_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Add L2 regularization gradient to weights (not biases)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4034840158.py\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3898644277.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(z)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloatNDArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFloatNDArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msigmoid_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFloatNDArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFloatNDArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}