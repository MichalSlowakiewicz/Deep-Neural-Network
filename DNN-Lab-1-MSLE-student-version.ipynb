{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalSlowakiewicz/Deep-Neural-Network/blob/master/DNN-Lab-1-MSLE-student-version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vzBJ0uoPuVX"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/MichalSlowakiewicz/Deep-Neural-Network/blob/master/DNN-Lab-1-MSLE-student-version.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
        "\n",
        "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
        "\n",
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ],
      "metadata": {
        "id": "Y676Rk9vA52c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqdiMxZx-EoD"
      },
      "source": [
        "# Linear regression\n",
        "\n",
        "In this exercise, you will use linear regression to predict flat (apartment) prices. Training will be handled via gradient descent. We will:\n",
        "* have multiple features (i.e. variables used to make the prediction),\n",
        "* employ some basic feature engineering,\n",
        "* work with a non-standard loss function.\n",
        "\n",
        "Let's start by obtaining the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI9wFXv5-EoP",
        "outputId": "0c7d8870-8f57-442b-cd5f-c0468f4a3933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --no-verbose -O mieszkania.csv https://www.dropbox.com/s/zey0gx91pna8irj/mieszkania.csv?dl=1\n",
        "!wget --no-verbose -O mieszkania_test.csv https://www.dropbox.com/s/dbrj6sbxb4ayqjz/mieszkania_test.csv?dl=1\n",
        "!head mieszkania.csv mieszkania_test.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-10 08:37:17 URL:https://uc05e563458998c7a4b918818e25.dl.dropboxusercontent.com/cd/0/inline/Cy-q42FYJcotvGB4Iat1vppwNqnyAfjefOKyYfK8lSyZ_e_CMVmNuwVJN8GRILS8VSpqpLQm0NmVB2PJQQgNr-hHx3zO__9d-QOXwkB--gE6xdP8dKaG5W--y_vrIHePkkU/file?dl=1 [6211/6211] -> \"mieszkania.csv\" [1]\n",
            "2025-10-10 08:37:20 URL:https://uce37a8183aca63439f4635eb03b.dl.dropboxusercontent.com/cd/0/inline/Cy8xS75Pvu1eqO4P_UoekcAFijXYQ0Vek-2lpt2kyrBIlNY-OwTGZCzq03Ee2FC70ye68nmWYSIx4EDW0H60RiPT2DqN9NE9Ma-U1l-xy8YY5BJ5djrBl6192rsnTLGSNjU/file?dl=1 [6247/6247] -> \"mieszkania_test.csv\" [1]\n",
            "==> mieszkania.csv <==\n",
            "m2,dzielnica,ilość_sypialni,ilość_łazienek,rok_budowy,parking_podziemny,cena\n",
            "104,mokotowo,2,2,1940,1,780094\n",
            "43,ochotowo,1,1,1970,1,346912\n",
            "128,grodziskowo,3,2,1916,1,523466\n",
            "112,mokotowo,3,2,1920,1,830965\n",
            "149,mokotowo,3,3,1977,0,1090479\n",
            "80,ochotowo,2,2,1937,0,599060\n",
            "58,ochotowo,2,1,1922,0,463639\n",
            "23,ochotowo,1,1,1929,0,166785\n",
            "40,mokotowo,1,1,1973,0,318849\n",
            "\n",
            "==> mieszkania_test.csv <==\n",
            "m2,dzielnica,ilość_sypialni,ilość_łazienek,rok_budowy,parking_podziemny,cena\n",
            "71,wolowo,2,2,1912,1,322227\n",
            "45,mokotowo,1,1,1938,0,295878\n",
            "38,mokotowo,1,1,1999,1,306530\n",
            "70,ochotowo,2,2,1980,1,553641\n",
            "136,mokotowo,3,2,1939,1,985348\n",
            "128,wolowo,3,2,1983,1,695726\n",
            "23,grodziskowo,1,1,1975,0,99751\n",
            "117,mokotowo,3,2,1942,0,891261\n",
            "65,ochotowo,2,1,2002,1,536499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH8Ut02G-EoU"
      },
      "source": [
        "Each row in the data represents a separate flat. Our goal is to use the data from `mieszkania.csv` to create a model that can predict a flat's price (i.e. `cena`) given its features (i.e. `m2,dzielnica,ilosc_sypialni,...`).\n",
        "\n",
        "We should use only `mieszkania.csv` (dubbed the training dataset) to make our decisions and create the model. The (only) purpose of `mieszkania_test.csv` is to test our model on **unseen** data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "NDArray = np.ndarray[Any, Any]\n",
        "\n",
        "np.set_printoptions(precision=4, suppress=True)\n",
        "np.random.seed(357)"
      ],
      "metadata": {
        "id": "r_F0mDKdWNp4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and converting data\n",
        "\n",
        "Let's start by loading the data and showing the range of prices we're working with."
      ],
      "metadata": {
        "id": "O4eyhKJKom2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load(path: str) -> tuple[NDArray, NDArray]:\n",
        "    \"\"\"\n",
        "    Returns (x, y) where:\n",
        "    - x: input features, shape (n_apartments, n_features)\n",
        "    - y: price, shape (n_apartments,)\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(path)\n",
        "    y = data[\"cena\"].to_numpy()\n",
        "    x = data.loc[:, data.columns != \"cena\"].to_numpy()\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "NjwPnwzoWNs9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = load(\"mieszkania.csv\")\n",
        "x_test, y_test = load(\"mieszkania_test.csv\")\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "rg-nV6_1WRk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a98b77-d3cc-48d2-d758-f86cc4bb276e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 6) (200,)\n",
            "(200, 6) (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(y_train), np.max(y_train), np.mean(y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx5b_IpiZBNi",
        "outputId": "ce22a9c2-fa0b-422b-96c6-cecbd9701afc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102572 1102309 507919.49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_oE2g2Yc9rt",
        "outputId": "a7950589-e2ec-4485-c916-ff96d693999c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[104, 'mokotowo', 2, 2, 1940, 1],\n",
              "       [43, 'ochotowo', 1, 1, 1970, 1],\n",
              "       [128, 'grodziskowo', 3, 2, 1916, 1]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to convert features to floats."
      ],
      "metadata": {
        "id": "Js0uXZK-c_NA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert column 1 from str to (ordinal) int.\n",
        "# (One-hot encoding would be better, but ordinal is OK for today.)\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(x_train[:, 1])\n",
        "x_train[:, 1] = label_encoder.transform(x_train[:, 1])\n",
        "x_test[:, 1] = label_encoder.transform(x_test[:, 1])\n",
        "\n",
        "# Convert ints to float.\n",
        "x_train = x_train.astype(np.float64)\n",
        "x_test = x_test.astype(np.float64)"
      ],
      "metadata": {
        "id": "QVTpIDKuWRq6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggvlLt-Tc3bX",
        "outputId": "b2134f72-d915-467a-9023-c080452b16ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 104.,    1.,    2.,    2., 1940.,    1.],\n",
              "       [  43.,    2.,    1.,    1., 1970.,    1.],\n",
              "       [ 128.,    0.,    3.,    2., 1916.,    1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8QX6Ncr-EoW"
      },
      "source": [
        "## The loss and constant models\n",
        "\n",
        "Our predictions should minimize the so-called *mean squared logarithmic error*:\n",
        "$$\n",
        "MSLE = \\frac{1}{n} \\sum_{i=1}^n (\\log(1+y_i) - \\log(1+p_i))^2,\n",
        "$$\n",
        "where $y_i$ is the ground truth, and $p_i$ is our prediction.\n",
        "\n",
        "Let's implement the loss function first."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(ys: NDArray, ps: NDArray) -> np.float64:\n",
        "    assert ys.shape == ps.shape\n",
        "    return np.mean((ys - ps) * (ys - ps))"
      ],
      "metadata": {
        "id": "3tjQSlPgWXRN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAiB8S9A-EoX"
      },
      "source": [
        "def msle(ys: NDArray, ps: NDArray) -> np.float64:\n",
        "    assert ys.shape == ps.shape\n",
        "    #################################\n",
        "    # TODO: Implement this function #\n",
        "    #################################\n",
        "    return np.mean((np.log(1+ys)-np.log(1+ps))**2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAmd2qLR-Eob"
      },
      "source": [
        "The simplest model is predicting the same constant for each instance. Test your implementation of msle against outputing the mean price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0RT7VW7-Eoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f2ecf9-8d01-473c-f27e-e63d0470557e"
      },
      "source": [
        "###################################################\n",
        "# TODO: Compute msle for outputing the mean price #\n",
        "###################################################\n",
        "y_mean = np.mean(y_train)\n",
        "msle_error =  msle(y_test, y_mean*np.ones(len(y_test)))\n",
        "print(msle_error)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4284115392580848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0zMmTiv-Eof"
      },
      "source": [
        "Recall that outputing the mean minimizes $MSE$. However, we're now dealing with $MSLE$.\n",
        "\n",
        "Think of a constant that should result in the lowest $MSLE$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Z4drNd-Eog",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a853f456-5e45-423b-acb1-40dff699cae3"
      },
      "source": [
        "#############################################\n",
        "# TODO: Find this constant and compute msle #\n",
        "#############################################\n",
        "\n",
        "y_mean = np.mean(np.log(y_train))\n",
        "y_mean = np.exp(y_mean)\n",
        "msle_error =  msle(y_test, np.ones(len(y_test))*y_mean)\n",
        "print(msle_error)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.42757528293624447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RdHlBw8-Eoi"
      },
      "source": [
        "## Linear regression (standard)\n",
        "\n",
        "Now, let's implement training of a standard linear regression model via gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn97D3vd-Eoj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156014d2-92af-44df-ff14-61c065bfc129"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def train(\n",
        "    x: NDArray, y: NDArray, alpha: float = 1e-7, n_iterations: int = 100000\n",
        ") -> tuple[NDArray, np.float64]:\n",
        "    \"\"\"Linear regression (which optimizes MSE). Returns (weights, bias).\"\"\"\n",
        "\n",
        "    # B is batch size (number of observations).\n",
        "    # F is number of (input) features.\n",
        "    B, F = x.shape\n",
        "    assert y.shape == (B,)\n",
        "\n",
        "    # TODO #\n",
        "    model = LinearRegression()\n",
        "    model.fit(x, y)\n",
        "    weights = model.coef_\n",
        "    bias = model.intercept_\n",
        "    return weights, bias\n",
        "\n",
        "weights, bias = train(x_train, y_train)\n",
        "preds_test =  x_test @ np.transpose(weights) + bias\n",
        "print(\"test MSLE:\", msle(y_test, preds_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test MSLE: 0.07098684418438012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with gradient descendant\n",
        "\n",
        "def train(\n",
        "    x: NDArray, y: NDArray, alpha: float = 1e-7, n_iterations: int = 100000\n",
        ") -> tuple[NDArray, np.float64]:\n",
        "    \"\"\"Linear regression (which optimizes MSE). Returns (weights, bias).\"\"\"\n",
        "\n",
        "    # B is batch size (number of observations).\n",
        "    # F is number of (input) features.\n",
        "    B, F = x.shape\n",
        "    assert y.shape == (B,)\n",
        "\n",
        "    weights = np.zeros(F)\n",
        "    bias = 0.0\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "      y_pred = x @ weights + bias\n",
        "\n",
        "      error = y_pred - y\n",
        "      grad_w = (2/B) * (x.T @ error)\n",
        "      grad_b = (2/B) * np.sum(error)\n",
        "\n",
        "      weights -= alpha * grad_w\n",
        "      bias -= alpha * grad_b\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "weights, bias = train(x_train, y_train)\n",
        "preds_test =  x_test @ np.transpose(weights) + bias\n",
        "print(\"test MSLE:\", msle(y_test, preds_test))"
      ],
      "metadata": {
        "id": "LpXIVAmRQK_9",
        "outputId": "d8ede770-606e-4c98-9ee9-b83feb563a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test MSLE: 0.08038067988623034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression (MSLE)"
      ],
      "metadata": {
        "id": "cfMn93ejteWt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzrrOte5-Eol"
      },
      "source": [
        "Note that the loss function that the algorithms optimizes (i.e $MSE$) differs from $MSLE$. We've already seen that this may result in a suboptimal solution.\n",
        "\n",
        "How can you change the setting so that we optimze $MSLE$ instead?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVD8kWEJ-Eom"
      },
      "source": [
        "Hint:\n",
        "<sub><sup><sub><sup><sub><sup>\n",
        "Be lazy. We don't want to change the algorithm.\n",
        "Use the chain rule and previous computations to get formulas for the gradient.\n",
        "</sup></sub></sup></sub></sup></sub>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_msle(\n",
        "    x: NDArray, y: NDArray, alpha: float = 1e-7, n_iterations: int = 100000\n",
        ") -> tuple[NDArray, np.float64]:\n",
        "    \"\"\"Linear regression (which optimizes MSE). Returns (weights, bias).\"\"\"\n",
        "\n",
        "    # B is batch size (number of observations).\n",
        "    # F is number of (input) features.\n",
        "    B, F = x.shape\n",
        "    assert y.shape == (B,)\n",
        "\n",
        "    weights = np.zeros(F)\n",
        "    bias = 0.0\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "      y_pred = x @ weights + bias\n",
        "\n",
        "      msle_loss = msle(y, y_pred)\n",
        "      grad_preds = (2/B) * (np.log(1+y_pred) -np.log(1+y))/(1+y_pred)\n",
        "      grad_b = np.sum(grad_preds)\n",
        "      grad_w = x.T @ grad_preds\n",
        "\n",
        "      \"\"\"log_diff = np.log1p(y) - np.log1p(y_pred)\n",
        "      dL_dy_pred = -2 * log_diff / (1 + y_pred)\n",
        "\n",
        "\n",
        "      grad_w = (1 / B) * (x.T @ dL_dy_pred)\n",
        "      grad_b = (1 / B) * np.sum(dL_dy_pred)\"\"\"\n",
        "\n",
        "      weights -= alpha * grad_w\n",
        "      bias -= alpha * grad_b\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "\n",
        "weights, bias = train_msle(x_train, y_train)\n",
        "preds_test =  x_test @ weights + bias\n",
        "print(\"test MSLE:\", msle(y_test, preds_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b32Ph3ha1ho",
        "outputId": "33c67b14-059b-4f3b-969b-c55b53e6c105"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test MSLE: 38.03557476309587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature engineering"
      ],
      "metadata": {
        "id": "7yfrbDbDrTns"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvWrBkLu-Eoo"
      },
      "source": [
        "Without any feature engineering our model approximates the price as a linear combination of original features:\n",
        "$$\n",
        "\\text{price} \\approx w_1 \\cdot \\text{area} + w_2 \\cdot \\text{district} + \\dots.\n",
        "$$\n",
        "Let's now introduce some interactions between the variables. For instance, let's consider a following formula:\n",
        "$$\n",
        "\\text{price} \\approx w_1 \\cdot \\text{area} \\cdot \\text{avg. price in the district per sq. meter} + w_2 \\cdot \\dots + \\dots.\n",
        "$$\n",
        "Here, we model the price with far greater granularity, and we may expect to see more acurate results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBM60E7t-Eop"
      },
      "source": [
        "Add some feature engineering to your model. Be sure to play with the data and not with the algorithm's code.\n",
        "\n",
        "Think how to make sure that your model is capable of capturing the $w_1 \\cdot \\text{area} \\cdot \\text{avg. price...}$ part, without actually computing the averages.\n",
        "\n",
        "Note that you may need to change the learning rate substantially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT5DziNC-Eoq"
      },
      "source": [
        "Hint:\n",
        "<sub><sup><sub><sup><sub><sup>\n",
        "Is having a binary encoding for each district and multiplying it by area enough?\n",
        "</sup></sub></sup></sub></sup></sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKOVCHNz-Eor"
      },
      "source": [
        "Hint 2:\n",
        "<sub><sup><sub><sup><sub><sup>\n",
        "Why not multiply everything together? I.e. (A,B,C) -> (AB,AC,BC).\n",
        "</sup></sub></sup></sub></sup></sub>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################\n",
        "# TODO: Implement the feature engineering part #\n",
        "###############################################\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"mieszkania.csv\")\n",
        "test = pd.read_csv(\"mieszkania_test.csv\")\n",
        "\n",
        "# Separate target\n",
        "y_train = train[\"cena\"].values\n",
        "train = train.drop(columns=[\"cena\"])\n",
        "y_test = test[\"cena\"].values\n",
        "test = test.drop(columns=[\"cena\"])\n",
        "\n",
        "# One-hot encode categorical variable\n",
        "enc = OneHotEncoder(sparse_output=False, drop=\"first\")\n",
        "districts_train = enc.fit_transform(train[[\"dzielnica\"]])\n",
        "districts_test = enc.transform(test[[\"dzielnica\"]])\n",
        "\n",
        "district_names = enc.get_feature_names_out([\"dzielnica\"])\n",
        "\n",
        "# Drop original column and add encoded\n",
        "train_num = train.drop(columns=[\"dzielnica\"])\n",
        "test_num = test.drop(columns=[\"dzielnica\"])\n",
        "\n",
        "X_train = np.concatenate([train_num.values, districts_train], axis=1)\n",
        "X_test = np.concatenate([test_num.values, districts_test], axis=1)\n",
        "\n",
        "# Convert back to DataFrame for easier manipulation\n",
        "columns = list(train_num.columns) + list(district_names)\n",
        "X_train = pd.DataFrame(X_train, columns=columns)\n",
        "X_test = pd.DataFrame(X_test, columns=columns)\n",
        "\n",
        "###############################################\n",
        "# ✅ FEATURE ENGINEERING SECTION\n",
        "###############################################\n",
        "\n",
        "# Interaction: area * district dummies\n",
        "for d in district_names:\n",
        "    X_train[f\"m2_{d}\"] = X_train[\"m2\"] * X_train[d]\n",
        "    X_test[f\"m2_{d}\"] = X_test[\"m2\"] * X_test[d]\n",
        "\n",
        "# Other interactions\n",
        "X_train[\"m2_sypialnie\"] = X_train[\"m2\"] * X_train[\"ilość_sypialni\"]\n",
        "X_train[\"m2_parking\"] = X_train[\"m2\"] * X_train[\"parking_podziemny\"]\n",
        "\n",
        "X_test[\"m2_sypialnie\"] = X_test[\"m2\"] * X_test[\"ilość_sypialni\"]\n",
        "X_test[\"m2_parking\"] = X_test[\"m2\"] * X_test[\"parking_podziemny\"]\n",
        "\n",
        "###############################################\n",
        "# Now you can feed X_train, y_train into your gradient descent\n",
        "###############################################\n",
        "\n",
        "x_train = X_train.values\n",
        "x_test = X_test.values\n"
      ],
      "metadata": {
        "id": "IgfbFqwgEXdf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Lj1TPr-Eot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bee6be1e-c4f9-422f-f8dd-e615d4cfcfcb"
      },
      "source": [
        "##############################################################\n",
        "# TODO: Test your solution on the training and test datasets #\n",
        "##############################################################\n",
        "\n",
        "def train(\n",
        "    x: NDArray, y: NDArray, alpha: float = 1e-7, n_iterations: int = 100000\n",
        ") -> tuple[NDArray, np.float64]:\n",
        "    \"\"\"Linear regression (which optimizes MSE). Returns (weights, bias).\"\"\"\n",
        "\n",
        "    # B is batch size (number of observations).\n",
        "    # F is number of (input) features.\n",
        "    B, F = x.shape\n",
        "    assert y.shape == (B,)\n",
        "\n",
        "    weights = np.zeros(F)\n",
        "    bias = 0.0\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "      y_pred = x @ weights + bias\n",
        "\n",
        "      error = y_pred - y\n",
        "      grad_w = (2/B) * (x.T @ error)\n",
        "      grad_b = (2/B) * np.sum(error)\n",
        "\n",
        "      weights -= alpha * grad_w\n",
        "      bias -= alpha * grad_b\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "weights, bias = train(\n",
        "    x_train,\n",
        "    y_train\n",
        ")\n",
        "\n",
        "# Predykcje\n",
        "preds_train = x_train @ weights + bias\n",
        "preds_test = x_test @ weights + bias\n",
        "\n",
        "\n",
        "\n",
        "# Obliczenie MSLE (Mean Squared Logarithmic Error)\n",
        "def msle(y_true, y_pred):\n",
        "    return np.mean((np.log1p(y_true) - np.log1p(y_pred)) ** 2)\n",
        "\n",
        "# Ewaluacja\n",
        "train_msle = msle(y_train, preds_train)\n",
        "test_msle = msle(y_test, preds_test)\n",
        "\n",
        "print(f\"Train MSLE: {train_msle:.6f}\")\n",
        "print(f\"Test  MSLE: {test_msle:.6f}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train MSLE: 0.008397\n",
            "Test  MSLE: 0.018653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "vdlipjGexuWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise you will implement a validation pipeline: split the non-test set into train and validation sets and select the best model based on validation results.\n",
        "\n",
        "So far you tested your model against the training and test datasets. As you should observe, there's a gap between the results. By validating your model, you should be able to better anticipate the test time performance and compare different models and hyperparameters on datasets they are not over-fitted to.\n",
        "\n",
        "Implement the basic validation method, i.e. a random split. Test it with your model from Exercise MSLE."
      ],
      "metadata": {
        "id": "5P1m1Bi1yEs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_val, y_train_val = x_train, y_train\n",
        "x_test, y_test = x_test, y_test\n",
        "\n",
        "\n",
        "def random_split(\n",
        "    x: NDArray, y: NDArray, val_ratio: float = 0.2\n",
        ") -> tuple[tuple[NDArray, NDArray], tuple[NDArray, NDArray]]:\n",
        "    \"\"\"Returns (x_train, y_train), (x_val, y_val).\"\"\"\n",
        "\n",
        "    idxs = np.random.permutation(len(x))\n",
        "\n",
        "    ######################################################\n",
        "    # TODO: Implement the basic validation split method. #\n",
        "    ######################################################\n",
        "    val_size = int(len(x) * val_ratio)\n",
        "\n",
        "    val_idxs = idxs[:val_size]\n",
        "    train_idxs = idxs[val_size:]\n",
        "\n",
        "    # Tworzymy zbiory\n",
        "    x_train, y_train = x[train_idxs], y[train_idxs]\n",
        "    x_val, y_val = x[val_idxs], y[val_idxs]\n",
        "\n",
        "    return (x_train, y_train), (x_val, y_val)\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_val, y_val) = random_split(x_train_val, y_train_val)\n",
        "\n",
        "len(x_train), len(x_val), len(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLvtN4Puxwu3",
        "outputId": "c78e5263-f092-415b-bc16-bcae74e5670c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(160, 40, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################################\n",
        "# TODO: compare MSLE on training, validation, and test sets #\n",
        "#############################################################"
      ],
      "metadata": {
        "id": "CMc4r9oWHSZN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validation"
      ],
      "metadata": {
        "id": "LImWR9ki69T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the random split validation reliable, a significant chunk of training data may be needed. To get over this problem, one may apply cross-validation.\n",
        "\n",
        "![alt-text](https://chrisjmccormick.files.wordpress.com/2013/07/10_fold_cv.png)\n",
        "\n",
        "Let's now implement the method. Make sure that:\n",
        "* number of partitions is a parameter,\n",
        "* the method is not limited to `mieszkania.csv`,\n",
        "* the method is not limited to one specific model."
      ],
      "metadata": {
        "id": "sWX_Soa25XJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "# TODO: Implement cross-validation #\n",
        "####################################\n",
        "def kfold(x: NDArray, y: NDArray, n_folds: int = 5, shuffle: bool = False) -> list[float]:\n",
        "    \"\"\"Returns losses for each fold.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "losses = kfold(x_train_val, y_train_val, n_folds=3, shuffle=False)\n",
        "print(f\"k-fold loss: {np.mean(losses):.4f} +- {np.std(losses):.4f}\")\n"
      ],
      "metadata": {
        "id": "ma_OGKv80rln",
        "outputId": "661b6b61-a982-4c3d-be91-cf0ceb391af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported operand type(s) for /: 'NoneType' and 'int'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1359811898.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"k-fold loss: {np.mean(losses):.4f} +- {np.std(losses):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3594\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3596\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3597\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Investigating input data"
      ],
      "metadata": {
        "id": "Rd7h-hJl7Q_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that sometimes validation may be tricky, e.g. significant class imbalance, having a small number of subjects, geographically clustered instances...\n",
        "\n",
        "What could in theory go wrong here with random, unstratified partitions? Think about potential solutions and investigate the data in order to check whether these problems arise here."
      ],
      "metadata": {
        "id": "jFeQiTQc7Rid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "# TODO: Investigate the data #\n",
        "##############################"
      ],
      "metadata": {
        "id": "1JQKK6wZ7SAi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}